{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro and summary of script\n",
    "\n",
    "When PMF finalists access the Talent Management System (TMS), they eventually reach a webpage with a table of every currently open position. Each row doubles as a javascript button, which when clicked brings the finalist to a page with the full description of that particular job.\n",
    "\n",
    "Using a web browsers 'inspect element' function, you can quickly see a pattern in those buttons. Each is uniquely named after the opportunity number (eg 217 for posting PMF-2017-0217). Because of that pattern, a webscraper can easily be written to 'click' each button in turn. Once a button is clicked, the HTML from the page with the detailed description may then be scraped for local storage.\n",
    "\n",
    "This notebook has 2 basic parts. The first part uses the webscraper Selenium to gather PMF position information for local storage. The script is not purely automated. When Selenium initially opens the url, the user must log into the TMS, navigate to the page with the table listing every opportunity, and set the page to display all available jobs. Additionally, a table of the available jobs to be saved as an excel file before running the scraping portion of the script.\n",
    "\n",
    "During scraping the HTMl content of each individual job posting is saved in a Pandas dataframe. Afterwards, some clunky code pulls out useful bits of information (job title, location, closing date, etc) into different columns in the dataframe before saving the dataframe as a CSV file locally.\n",
    "\n",
    "The 2nd part of this notebook opens that locally saved CSV file and does some additional processing. Most importantly, it breaks up each posting by locations. For example, if an opportunity has 3 available positions in DC and 1 available position in CO, this section of the script breaks those into 2 different rows. A bit of additional code does a boolean test for the possible starting grade level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a Python notebook, I will be standing on the shoulders of giants by using clever packages and modules others have built. I import them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver #selenium is used to interact with the webpage, so the program can 'click' buttons.\n",
    "#also make sure to have the application geckodriver in system path. Selenium needs this to function.\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary #This will let the program open the webpage on a new Firefox window.\n",
    "import sys #This is only used to assign a location to my path. The location of the application geckodriver.\n",
    "from bs4 import BeautifulSoup #BeautifulSoup is used to parse the HTML of the downloaded website to find the particular information desired.\n",
    "import time #I will need to delay the program to give the webpage time to open. time will be used for that.\n",
    "import pandas as pd #the data will be saved locally as a csv file. Pandas is a nice way to write/read/work with those files.\n",
    "import numpy as np #to do math\n",
    "import matplotlib.pyplot as plt #To make plots\n",
    "import seaborn as sns #To make plots in a different way\n",
    "from datetime import timedelta #for accurate plotting of dates%matplotlib inline\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scraping the data\n",
    "\n",
    "This script uses the selenium package to interact with javascript buttons in an opened firefox browser window. Something that is not always obvious in the selenium docs is that a secondary application, geckodriver, needs to be in the system path to have selenium work properly. This first little bit of code just makes sure that the location of geckodriver is in the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path\n",
    "#sys.path.append('C:\\\\Users...\\\\folder_with_geckodriver')\n",
    "sys.path.append('C:\\\\Users\\\\Kyle\\\\Documents\\\\Blog Posts\\\\PMF\\\\PMF positions')\n",
    "#If this fails, try putting geckodriver in same working folder as notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can have Selenium open a Firefox browser using the url of the PMF TMS listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='https://apply.pmf.gov/opportunity_search_results.aspx'\n",
    "binary = FirefoxBinary('C:\\\\Program Files (x86)\\\\Mozilla Firefox\\\\firefox.exe')\n",
    "driver = webdriver.Firefox(firefox_binary=binary)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everying is behaving correctly, a new Firefox browser window should have opened at the log in page for the TMS. Before continuing script, go to that browser window, log in, navigate to the table of listings, and set up the table to display all open postings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block scrapes the opportunity numbers from the TMS and records the date of the scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = driver.page_source #downloading html of individual job description\n",
    "soup = BeautifulSoup(html,'lxml') #decoding the html into a workable form.\n",
    "a=soup.find(id='container_date').text.replace('\\xa0','')\n",
    "b=datetime.strptime(a,'%A, %B %d, %Y')\n",
    "date=b.strftime('%y%m%d')\n",
    "keys=[];\n",
    "a=soup.find('main')\n",
    "for elm in a.find_all('a'):\n",
    "    if 'PMF-2017-' in elm.text:\n",
    "        keys.append(int(elm.text[-4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to let this script interact with the TMS. \n",
    "\n",
    "The first step is to initialize a pandas dataframe with the information from the keys. This dataframe will be where the HTMl of each posting's individual webpage is also saved.\n",
    "\n",
    "The for loop scrapes the data, jumping back and forth between the listings and individual job postings, and with some delay built in to let the browser finish opening a new page before the script continues. You can watch the Firefox browser window while this is happening, which can be very satisfying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['listing']=keys\n",
    "df['info']='1'\n",
    "for elm in keys:\n",
    "    #below: each javascript button associated with every row of the TMS is named after the job number\n",
    "    #This for loop 'clicks' on each button in turn by making use of that pattern.\n",
    "    driver.find_element_by_xpath('//a[@href=\"javascript:goSubmit(%s);\"]'%(int(elm))).click()\n",
    "    time.sleep(2) #delay to let the page load\n",
    "    html = driver.page_source #downloading html of individual job description\n",
    "    soup = BeautifulSoup(html,'lxml') #decoding the html into a workable form.\n",
    "    data=soup.find('main') #side stepping all the CSS information from the webpage\n",
    "    test=[]\n",
    "    #below: this little loop finds the job description part of the main HTML section and\n",
    "    #saves it as a string in df\n",
    "    for row in data:\n",
    "        if 'Announcement Number' in str(row):\n",
    "            test.append(row)\n",
    "    df.loc[df.loc[:,'listing']==elm,'info']=str(test[0])\n",
    "    driver.find_element_by_xpath('//button[text()=\"Back\"]').click() #return to TMS table and prepare to repeat.\n",
    "    time.sleep(2) #delay to let the page load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the data is locally captured, although the HTML formal is not very easy to read.\n",
    "\n",
    "This online service will convert that html (first copy/paste into a notepad and save as .html) into pdf with only minor transcription errors: http://pdfcrowd.com/#convert_by_upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of clunky code pulls out some of the standard information into new columns for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test=pd.DataFrame();\n",
    "#test['label']=range(0,10)\n",
    "for x in range(0,len(df)):\n",
    "    a=BeautifulSoup(df.loc[x,'info'], \"html.parser\")\n",
    "    b=a.text.split('\\n')\n",
    "    df.loc[x,'Position Title']=b[b.index('Position Title')+1]\n",
    "    df.loc[x,'Agency/Sub-Agency']=b[b.index('Agency/Sub-Agency')+1]\n",
    "    df.loc[x,'Opening Date']=b[b.index('Opening Date')+1]\n",
    "    df.loc[x,'Closing Date']=b[b.index('Closing Date')+1]\n",
    "    df.loc[x,'Positions']=b[b.index('Number of Positions and Location(s)')+1:b.index('Job Series and Occupational Group')]\n",
    "    df.loc[x,'Series']=b[b.index('Job Series and Occupational Group')+1]\n",
    "    df.loc[x,'Grade Level']=b[b.index('Pay Plan, Grade Level, and Salary Range')+1:b.index('Promotion Potential')]\n",
    "    df.loc[x,'Promotion Potential']=b[b.index('Promotion Potential')+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing</th>\n",
       "      <th>info</th>\n",
       "      <th>Position Title</th>\n",
       "      <th>Agency/Sub-Agency</th>\n",
       "      <th>Opening Date</th>\n",
       "      <th>Closing Date</th>\n",
       "      <th>Positions</th>\n",
       "      <th>Series</th>\n",
       "      <th>Grade Level</th>\n",
       "      <th>Promotion Potential</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>454</td>\n",
       "      <td>&lt;div class=\"row\"&gt;\\n&lt;div class=\"col-sm-10\"&gt;\\n&lt;d...</td>\n",
       "      <td>Budget Analyst, GS9/11</td>\n",
       "      <td>Department of Justice / Federal Bureau of Inve...</td>\n",
       "      <td>4/26/2017</td>\n",
       "      <td>4/30/2017</td>\n",
       "      <td>[1 in Washington DC, DC]</td>\n",
       "      <td>0560 - Budget Analysis</td>\n",
       "      <td>[GS-09, GS-11$54,972 - $86,460]</td>\n",
       "      <td>GS-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>455</td>\n",
       "      <td>&lt;div class=\"row\"&gt;\\n&lt;div class=\"col-sm-10\"&gt;\\n&lt;d...</td>\n",
       "      <td>Computer Scientist</td>\n",
       "      <td>Department of the Treasury / Internal Revenue ...</td>\n",
       "      <td>4/26/2017</td>\n",
       "      <td>4/30/2017</td>\n",
       "      <td>[1 in Washington DC, DC]</td>\n",
       "      <td>1550 - Computer Science</td>\n",
       "      <td>[GS-12$79,720 - $103,639]</td>\n",
       "      <td>GS-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>457</td>\n",
       "      <td>&lt;div class=\"row\"&gt;\\n&lt;div class=\"col-sm-10\"&gt;\\n&lt;d...</td>\n",
       "      <td>Social Scientist</td>\n",
       "      <td>Department of the Treasury / Internal Revenue ...</td>\n",
       "      <td>4/26/2017</td>\n",
       "      <td>4/30/2017</td>\n",
       "      <td>[1 in Washington DC, DC]</td>\n",
       "      <td>0101 - Social Science</td>\n",
       "      <td>[GS-12$79,720 - $123,234]</td>\n",
       "      <td>GS-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing                                               info  \\\n",
       "36      454  <div class=\"row\">\\n<div class=\"col-sm-10\">\\n<d...   \n",
       "37      455  <div class=\"row\">\\n<div class=\"col-sm-10\">\\n<d...   \n",
       "38      457  <div class=\"row\">\\n<div class=\"col-sm-10\">\\n<d...   \n",
       "\n",
       "            Position Title                                  Agency/Sub-Agency  \\\n",
       "36  Budget Analyst, GS9/11  Department of Justice / Federal Bureau of Inve...   \n",
       "37      Computer Scientist  Department of the Treasury / Internal Revenue ...   \n",
       "38        Social Scientist  Department of the Treasury / Internal Revenue ...   \n",
       "\n",
       "   Opening Date Closing Date                 Positions  \\\n",
       "36    4/26/2017    4/30/2017  [1 in Washington DC, DC]   \n",
       "37    4/26/2017    4/30/2017  [1 in Washington DC, DC]   \n",
       "38    4/26/2017    4/30/2017  [1 in Washington DC, DC]   \n",
       "\n",
       "                     Series                      Grade Level  \\\n",
       "36   0560 - Budget Analysis  [GS-09, GS-11$54,972 - $86,460]   \n",
       "37  1550 - Computer Science        [GS-12$79,720 - $103,639]   \n",
       "38    0101 - Social Science        [GS-12$79,720 - $123,234]   \n",
       "\n",
       "   Promotion Potential  \n",
       "36               GS-14  \n",
       "37               GS-13  \n",
       "38               GS-13  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this dataframe is locally saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'170426'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('listingsSnapShot%s.csv'%(date),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
